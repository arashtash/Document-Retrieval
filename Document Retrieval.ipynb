{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pandas in /Users/arashtashakori/Library/Python/3.9/lib/python/site-packages (2.2.1)\n",
            "Requirement already satisfied: nltk in /Users/arashtashakori/Library/Python/3.9/lib/python/site-packages (3.8.1)\n",
            "Requirement already satisfied: whoosh in /Users/arashtashakori/Library/Python/3.9/lib/python/site-packages (2.7.4)\n",
            "Requirement already satisfied: scikit-learn in /Users/arashtashakori/Library/Python/3.9/lib/python/site-packages (1.4.0)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/arashtashakori/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/arashtashakori/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/arashtashakori/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/arashtashakori/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: click in /Users/arashtashakori/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/arashtashakori/Library/Python/3.9/lib/python/site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/arashtashakori/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /Users/arashtashakori/Library/Python/3.9/lib/python/site-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/arashtashakori/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.12.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/arashtashakori/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas nltk whoosh scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/arashtashakori/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/arashtashakori/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import time\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#WHOOSH\n",
        "from whoosh import index\n",
        "from whoosh.fields import Schema, TEXT, ID\n",
        "from whoosh.analysis import StemmingAnalyzer\n",
        "from whoosh.qparser import QueryParser, MultifieldParser\n",
        "from whoosh.index import create_in, open_dir\n",
        "from whoosh import scoring\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>directions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>Caramel Frosting</td>\n",
              "      <td>let butter melt add sugar brown add milk bring...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Barbecued Chicken Wings</td>\n",
              "      <td>mix sauce brown sugar onion water mixing bowl ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>Pound Cake</td>\n",
              "      <td>bake hour put toothpick fork middle done tooth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>Slush</td>\n",
              "      <td>combine sugar boiling water stir cool add bana...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>Granny Ebert'S Hash</td>\n",
              "      <td>cover meat water teaspoon salt teaspoon pepper...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         title  \\\n",
              "49995         Caramel Frosting   \n",
              "49996  Barbecued Chicken Wings   \n",
              "49997               Pound Cake   \n",
              "49998                    Slush   \n",
              "49999      Granny Ebert'S Hash   \n",
              "\n",
              "                                              directions  \n",
              "49995  let butter melt add sugar brown add milk bring...  \n",
              "49996  mix sauce brown sugar onion water mixing bowl ...  \n",
              "49997  bake hour put toothpick fork middle done tooth...  \n",
              "49998  combine sugar boiling water stir cool add bana...  \n",
              "49999  cover meat water teaspoon salt teaspoon pepper...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#First of all we need to load the data which is in the CSV format to a Pandas dataframe to be able to access\n",
        "#and manipulate it efficiently\n",
        "\n",
        "path = \"food_recipes.csv\"\n",
        "recipes = pd.read_csv(path)\n",
        "\n",
        "\n",
        "#Cleaning the text by converting to lower case and removing punctuations: Taken from Tutorial 1\n",
        "def clean_text (text):\n",
        "    #Lower case the given text\n",
        "    text = text.lower()\n",
        "\n",
        "    #Removing punctuation\n",
        "    text = re.sub(r'[^a-z]', ' ', text)\n",
        "    return text\n",
        "\n",
        "#Removing stopwords to focus on more meaningful words from a text: Taken from Tutorial 1\n",
        "def stopwords_removal(text):\n",
        "    #Getting the set of stopwords in English\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    #Making up the text again without stopwords\n",
        "    sentence = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return sentence\n",
        "\n",
        "#Turning every word in the given text into its more basic core word \"lemma\": Taken from Tutorial 1\n",
        "def lemmatize(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sentence = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    return sentence\n",
        "\n",
        "#This is the pre-processing function that performs text cleaning, removing stop words and lemmatization and drops\n",
        "#rows with empty entries\n",
        "def preprocess (data):\n",
        "    #While I couldn't find any empty entry it is important to drop rows with missing entries to make the code work\n",
        "    #according to our Data mining class last term\n",
        "    data = data.dropna()\n",
        "\n",
        "    #Applying the preprocessing steps using the functions defined above\n",
        "    data['directions'] = data['directions'].apply(clean_text)\n",
        "    data['directions'] = data['directions'].apply(stopwords_removal)\n",
        "    data['directions'] = data['directions'].apply(lemmatize)\n",
        "\n",
        "    return data\n",
        "    \n",
        "\n",
        "recipes = preprocess(recipes)\n",
        "\n",
        "#Displaying the last 5 entries of the dataframe\n",
        "recipes.tail()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the pre-processing step first we need to clean the data by turning all texts (descriptions) into lower case and removing all punctuations using the regular expression library of python as discussed in the tutorial. This ensures that upper case letters and punctuations that don't have a purpose in the search won't affect our search. Then we used the built-in stopwords library to get the stop words in English to remove them as they don't add any specific value for search purposes. Then we lemmatize the directions by turning every word into the base 'dictionary-like' word to unify words that have the same approximate meaning to prevent slight variations in forms of the same conceptual word affecting the search. Also, before all of that we drop all rows with empty fields as they don't have any value in our search and may cause exceptions later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Term-Document Incidence Matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART A:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            0      1      2      3      4      5      6      7      8      \\\n",
            "ability         0      0      0      0      0      0      0      0      0   \n",
            "able            0      0      0      0      0      0      0      0      0   \n",
            "ableskiver      0      0      0      0      0      0      0      0      0   \n",
            "absent          0      0      0      0      0      0      0      0      0   \n",
            "absolutely      0      0      0      0      0      0      0      0      0   \n",
            "...           ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
            "zita            0      0      0      0      0      0      0      0      0   \n",
            "ziti            0      0      0      0      0      0      0      0      0   \n",
            "zucchini        0      0      0      0      0      0      0      0      0   \n",
            "zuchini         0      0      0      0      0      0      0      0      0   \n",
            "zwieback        0      0      0      0      0      0      0      0      0   \n",
            "\n",
            "            9      ...  49990  49991  49992  49993  49994  49995  49996  \\\n",
            "ability         0  ...      0      0      0      0      0      0      0   \n",
            "able            0  ...      0      0      0      0      0      0      0   \n",
            "ableskiver      0  ...      0      0      0      0      0      0      0   \n",
            "absent          0  ...      0      0      0      0      0      0      0   \n",
            "absolutely      0  ...      0      0      0      0      0      0      0   \n",
            "...           ...  ...    ...    ...    ...    ...    ...    ...    ...   \n",
            "zita            0  ...      0      0      0      0      0      0      0   \n",
            "ziti            0  ...      0      0      0      0      0      0      0   \n",
            "zucchini        0  ...      0      0      0      0      0      0      0   \n",
            "zuchini         0  ...      0      0      0      0      0      0      0   \n",
            "zwieback        0  ...      0      0      0      0      0      0      0   \n",
            "\n",
            "            49997  49998  49999  \n",
            "ability         0      0      0  \n",
            "able            0      0      0  \n",
            "ableskiver      0      0      0  \n",
            "absent          0      0      0  \n",
            "absolutely      0      0      0  \n",
            "...           ...    ...    ...  \n",
            "zita            0      0      0  \n",
            "ziti            0      0      0  \n",
            "zucchini        0      0      0  \n",
            "zuchini         0      0      0  \n",
            "zwieback        0      0      0  \n",
            "\n",
            "[6343 rows x 50000 columns]\n"
          ]
        }
      ],
      "source": [
        "#For this I used CountVectorizer from sklearn's feature extraction library. I found details about using it on \n",
        "#Sklearn's website https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "#which I found by searching on Google: \"Token counting library Sklearn\". Please see Imports section above for the \n",
        "#importing statement.\n",
        "\n",
        "td_matrix_maker = CountVectorizer(binary=True) #Since we are doing binary search, the exact number is irrelavant\n",
        "td_incidence = td_matrix_maker.fit_transform(recipes[\"directions\"])    #Running the CountVectorizer on the DF\n",
        "\n",
        "#Turning the result into a dataframe of its own\n",
        "incidence_matrix_lib = pd.DataFrame(td_incidence.toarray(), columns=td_matrix_maker.get_feature_names_out()).T\n",
        "\n",
        "\n",
        "print(incidence_matrix_lib)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART B:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            0      1      2      3      4      5      6      7      8      \\\n",
            "ability         0      0      0      0      0      0      0      0      0   \n",
            "able            0      0      0      0      0      0      0      0      0   \n",
            "ableskiver      0      0      0      0      0      0      0      0      0   \n",
            "absent          0      0      0      0      0      0      0      0      0   \n",
            "absolutely      0      0      0      0      0      0      0      0      0   \n",
            "...           ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
            "zita            0      0      0      0      0      0      0      0      0   \n",
            "ziti            0      0      0      0      0      0      0      0      0   \n",
            "zucchini        0      0      0      0      0      0      0      0      0   \n",
            "zuchini         0      0      0      0      0      0      0      0      0   \n",
            "zwieback        0      0      0      0      0      0      0      0      0   \n",
            "\n",
            "            9      ...  49990  49991  49992  49993  49994  49995  49996  \\\n",
            "ability         0  ...      0      0      0      0      0      0      0   \n",
            "able            0  ...      0      0      0      0      0      0      0   \n",
            "ableskiver      0  ...      0      0      0      0      0      0      0   \n",
            "absent          0  ...      0      0      0      0      0      0      0   \n",
            "absolutely      0  ...      0      0      0      0      0      0      0   \n",
            "...           ...  ...    ...    ...    ...    ...    ...    ...    ...   \n",
            "zita            0  ...      0      0      0      0      0      0      0   \n",
            "ziti            0  ...      0      0      0      0      0      0      0   \n",
            "zucchini        0  ...      0      0      0      0      0      0      0   \n",
            "zuchini         0  ...      0      0      0      0      0      0      0   \n",
            "zwieback        0  ...      0      0      0      0      0      0      0   \n",
            "\n",
            "            49997  49998  49999  \n",
            "ability         0      0      0  \n",
            "able            0      0      0  \n",
            "ableskiver      0      0      0  \n",
            "absent          0      0      0  \n",
            "absolutely      0      0      0  \n",
            "...           ...    ...    ...  \n",
            "zita            0      0      0  \n",
            "ziti            0      0      0  \n",
            "zucchini        0      0      0  \n",
            "zuchini         0      0      0  \n",
            "zwieback        0      0      0  \n",
            "\n",
            "[6359 rows x 50000 columns]\n"
          ]
        }
      ],
      "source": [
        "#From tutorial 1: This is the tutorial implementation of the term_document incidence matrix creator. It finds whether a word exists in a series of entries or not. \"The term incidence matrix\"\n",
        "def term_document_incidence_matrix(data):\n",
        "    # initialize an empty set to store unique words\n",
        "    words = set()\n",
        "    \n",
        "    # iterate over the content column in the dataframe and update the set with unique words\n",
        "    for content in data:\n",
        "        words.update(content.split())\n",
        "    \n",
        "    # convert the set to a list and sort it\n",
        "    words = list(words)\n",
        "    words.sort()\n",
        "    \n",
        "    # define an empty list to store the matrix\n",
        "    matrix = []\n",
        "    \n",
        "    # create a row for each content in the dataframe \n",
        "    for content in data:\n",
        "        \n",
        "        # store the row as a list of zeros with the length of the unique words\n",
        "        row = [0] * len(words)\n",
        "        \n",
        "        # for each word in the content, update the row with 1 where the word is found\n",
        "        for word in content.split():\n",
        "            row[words.index(word)] = 1\n",
        "        \n",
        "        matrix.append(row)\n",
        "    \n",
        "    # return the matrix as a dataframe with the words as columns\n",
        "    return pd.DataFrame(matrix, columns=words).T\n",
        "\n",
        "tutorial_incidence_matrix = term_document_incidence_matrix(recipes[\"directions\"])\n",
        "print(tutorial_incidence_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART C:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SKLEARN LIBRARY IMPLEMENTATION:\n",
            "            0      1      2      3      4      5      6      7      8      \\\n",
            "ability         0      0      0      0      0      0      0      0      0   \n",
            "able            0      0      0      0      0      0      0      0      0   \n",
            "ableskiver      0      0      0      0      0      0      0      0      0   \n",
            "absent          0      0      0      0      0      0      0      0      0   \n",
            "absolutely      0      0      0      0      0      0      0      0      0   \n",
            "...           ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
            "zita            0      0      0      0      0      0      0      0      0   \n",
            "ziti            0      0      0      0      0      0      0      0      0   \n",
            "zucchini        0      0      0      0      0      0      0      0      0   \n",
            "zuchini         0      0      0      0      0      0      0      0      0   \n",
            "zwieback        0      0      0      0      0      0      0      0      0   \n",
            "\n",
            "            9      ...  49990  49991  49992  49993  49994  49995  49996  \\\n",
            "ability         0  ...      0      0      0      0      0      0      0   \n",
            "able            0  ...      0      0      0      0      0      0      0   \n",
            "ableskiver      0  ...      0      0      0      0      0      0      0   \n",
            "absent          0  ...      0      0      0      0      0      0      0   \n",
            "absolutely      0  ...      0      0      0      0      0      0      0   \n",
            "...           ...  ...    ...    ...    ...    ...    ...    ...    ...   \n",
            "zita            0  ...      0      0      0      0      0      0      0   \n",
            "ziti            0  ...      0      0      0      0      0      0      0   \n",
            "zucchini        0  ...      0      0      0      0      0      0      0   \n",
            "zuchini         0  ...      0      0      0      0      0      0      0   \n",
            "zwieback        0  ...      0      0      0      0      0      0      0   \n",
            "\n",
            "            49997  49998  49999  \n",
            "ability         0      0      0  \n",
            "able            0      0      0  \n",
            "ableskiver      0      0      0  \n",
            "absent          0      0      0  \n",
            "absolutely      0      0      0  \n",
            "...           ...    ...    ...  \n",
            "zita            0      0      0  \n",
            "ziti            0      0      0  \n",
            "zucchini        0      0      0  \n",
            "zuchini         0      0      0  \n",
            "zwieback        0      0      0  \n",
            "\n",
            "[6343 rows x 50000 columns]\n",
            "Tutorial Implementation:\n",
            "            0      1      2      3      4      5      6      7      8      \\\n",
            "ability         0      0      0      0      0      0      0      0      0   \n",
            "able            0      0      0      0      0      0      0      0      0   \n",
            "ableskiver      0      0      0      0      0      0      0      0      0   \n",
            "absent          0      0      0      0      0      0      0      0      0   \n",
            "absolutely      0      0      0      0      0      0      0      0      0   \n",
            "...           ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
            "zita            0      0      0      0      0      0      0      0      0   \n",
            "ziti            0      0      0      0      0      0      0      0      0   \n",
            "zucchini        0      0      0      0      0      0      0      0      0   \n",
            "zuchini         0      0      0      0      0      0      0      0      0   \n",
            "zwieback        0      0      0      0      0      0      0      0      0   \n",
            "\n",
            "            9      ...  49990  49991  49992  49993  49994  49995  49996  \\\n",
            "ability         0  ...      0      0      0      0      0      0      0   \n",
            "able            0  ...      0      0      0      0      0      0      0   \n",
            "ableskiver      0  ...      0      0      0      0      0      0      0   \n",
            "absent          0  ...      0      0      0      0      0      0      0   \n",
            "absolutely      0  ...      0      0      0      0      0      0      0   \n",
            "...           ...  ...    ...    ...    ...    ...    ...    ...    ...   \n",
            "zita            0  ...      0      0      0      0      0      0      0   \n",
            "ziti            0  ...      0      0      0      0      0      0      0   \n",
            "zucchini        0  ...      0      0      0      0      0      0      0   \n",
            "zuchini         0  ...      0      0      0      0      0      0      0   \n",
            "zwieback        0  ...      0      0      0      0      0      0      0   \n",
            "\n",
            "            49997  49998  49999  \n",
            "ability         0      0      0  \n",
            "able            0      0      0  \n",
            "ableskiver      0      0      0  \n",
            "absent          0      0      0  \n",
            "absolutely      0      0      0  \n",
            "...           ...    ...    ...  \n",
            "zita            0      0      0  \n",
            "ziti            0      0      0  \n",
            "zucchini        0      0      0  \n",
            "zuchini         0      0      0  \n",
            "zwieback        0      0      0  \n",
            "\n",
            "[6359 rows x 50000 columns]\n"
          ]
        }
      ],
      "source": [
        "print(\"SKLEARN LIBRARY IMPLEMENTATION:\")\n",
        "print (incidence_matrix_lib)\n",
        "print(\"Tutorial Implementation:\")\n",
        "print(tutorial_incidence_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART D:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This method uses boolean search to find the index of rows that contains given search terms: From tutorial 1\n",
        "def boolean_search(data, matrix, search_terms, operator='AND'):\n",
        "    try:\n",
        "        # Ensure that the search terms are in lowercase\n",
        "        search_terms = [term.lower() for term in search_terms]\n",
        "                \n",
        "        # Filter the matrix to include only the search terms\n",
        "        filtered_matrix = matrix.loc[search_terms]\n",
        "    \n",
        "        # Find all reviews where all terms appear (boolean AND operation)\n",
        "        if operator == 'AND':\n",
        "            valid_indices = filtered_matrix.columns[(filtered_matrix > 0).all()]\n",
        "            \n",
        "        # Find all reviews where any term appears (boolean OR operation)\n",
        "        elif operator == 'OR':\n",
        "            valid_indices = filtered_matrix.columns[(filtered_matrix > 0).any()]\n",
        "            \n",
        "        else:\n",
        "            raise ValueError(\"Operator must be 'AND' or 'OR'\")\n",
        "\n",
        "        # Select and return the relevant data using the valid indices\n",
        "        return data.loc[valid_indices, ['title', 'directions']]\n",
        "\n",
        "    except KeyError as e:\n",
        "\n",
        "        # Handle the case where one or more search terms are not in the matrix\n",
        "        print(f\"Warning: {str(e).strip('[]')} not found\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if any term is not found\n",
        "    \n",
        "\n",
        "search_terms = [\"sugar\", \"oil\", \"bowl\", \"milk\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                              title  \\\n",
            "29                   One Hour Rolls   \n",
            "86                      Funnel Cake   \n",
            "863              Apple-Banana Bread   \n",
            "1634              Golden Corn Bread   \n",
            "1841    Chocolate Chip Vanilla Gems   \n",
            "...                             ...   \n",
            "45342              Easy Yeast Rolls   \n",
            "47198               Cherry Nut Loaf   \n",
            "47775        Country Fair Egg Bread   \n",
            "49183  Lovelight Chiffon Layer Cake   \n",
            "49260           Orange Corn Muffins   \n",
            "\n",
            "                                              directions  \n",
            "29     put flour large mixing bowl combine sugar milk...  \n",
            "86     sift first ingredient together bowl mix egg mi...  \n",
            "863    combine egg banana sugar oil milk separate bow...  \n",
            "1634   preheat oven sift together corn meal flour sug...  \n",
            "1841   preheat oven combine flour sugar baking powder...  \n",
            "...                                                  ...  \n",
            "45342  combine yeast warm milk soften yeast minute co...  \n",
            "47198  sift flour salt soda powder large bowl mix che...  \n",
            "47775  heat c water c milk soften yeast warm water le...  \n",
            "49183  preheat oven set egg hour separate grease flou...  \n",
            "49260  mixing bowl combine cornmeal flour sugar salt ...  \n",
            "\n",
            "[87 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "result_lib = boolean_search(recipes, incidence_matrix_lib, search_terms, operator=\"AND\")\n",
        "print(result_lib)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                              title  \\\n",
            "29                   One Hour Rolls   \n",
            "86                      Funnel Cake   \n",
            "863              Apple-Banana Bread   \n",
            "1634              Golden Corn Bread   \n",
            "1841    Chocolate Chip Vanilla Gems   \n",
            "...                             ...   \n",
            "45342              Easy Yeast Rolls   \n",
            "47198               Cherry Nut Loaf   \n",
            "47775        Country Fair Egg Bread   \n",
            "49183  Lovelight Chiffon Layer Cake   \n",
            "49260           Orange Corn Muffins   \n",
            "\n",
            "                                              directions  \n",
            "29     put flour large mixing bowl combine sugar milk...  \n",
            "86     sift first ingredient together bowl mix egg mi...  \n",
            "863    combine egg banana sugar oil milk separate bow...  \n",
            "1634   preheat oven sift together corn meal flour sug...  \n",
            "1841   preheat oven combine flour sugar baking powder...  \n",
            "...                                                  ...  \n",
            "45342  combine yeast warm milk soften yeast minute co...  \n",
            "47198  sift flour salt soda powder large bowl mix che...  \n",
            "47775  heat c water c milk soften yeast warm water le...  \n",
            "49183  preheat oven set egg hour separate grease flou...  \n",
            "49260  mixing bowl combine cornmeal flour sugar salt ...  \n",
            "\n",
            "[87 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "result_tutorial = boolean_search(recipes, tutorial_incidence_matrix, search_terms, operator=\"AND\")\n",
        "print(result_tutorial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART E: I am going to answer this solely based on runtime. I have isolated execution of the code for the creation of the matrices and running the search algorithm to see how long each of them takes to execute. When it comes to creating the matrix the Sklearn library's implementation of the creation of the term incidence matrix SIGNIFICANTLY outperforms the tutorial's barebone implementation. It took 1 minutes and 53 seconds for the tutorial barebone implementation to make the matrix while the sklearn library's implementation took only 0.9 second. That is the creation of the matrix with the library was 125 times faster using the library. The boolean search however was very close with the tutorial's barebone implementation taking 0.1 seconds vs sklearn's matrix taking 0.6 seconds. Still the massive difference between how long it takes to make the matrix in the first place using the tutorial's barebone implementation of the term incidence matrix, makes sklearn far more efficient for the data given for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inverted Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Maps the words to document indexes and return the mapped dictionary\n",
        "def map_function(data):\n",
        "    mapped_dict = {}\n",
        "    for index, row in data.iterrows():\n",
        "        words = row['directions'].split() #Break into array of single words\n",
        "    \n",
        "        for word in words:\n",
        "            if word in mapped_dict:    #If word is already in the dictionary append it to the appropriate index\n",
        "                mapped_dict[word].append(index)    \n",
        "            else:   #If not create a new index with word as key\n",
        "                mapped_dict[word] = [index]\n",
        "    return mapped_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Gets the mapped dictionary from map_function and reduce it to a final inverted index where each word is associated\n",
        "#with the set of document indexes in which the word appears by reducing the processed dictionary of key words and index values\n",
        "def reduce_function(mapped_dict): \n",
        "    inverted_index = {}\n",
        "    for word, indices in mapped_dict.items():\n",
        "        if word in inverted_index:  #If the word is already in the final inverted matrix\n",
        "            inverted_index[word].update(indices)    #update set of document indexes for that word\n",
        "        else:   #If the word doesn't exist\n",
        "            inverted_index[word] = set(indices) #Initialize a New set of indixes\n",
        "    return inverted_index\n",
        "# write the implementation for the map function here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This just uses the last two functions to create the inverted index by feeding the mapped dictionary of words into\n",
        "#the reducer function\n",
        "def create_inverted_index_map_reduce(data):\n",
        "    # map the data\n",
        "    mapped_data = map_function(data)\n",
        "    # reduce the mapped dictionary to get the inverted index\n",
        "    inverted_index = reduce_function(mapped_data)\n",
        "    return inverted_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inverted_index_search(recipe_data, inverted_index, search_terms):\n",
        "    try:\n",
        "        # Find all indices where all terms appear (boolean AND operation)\n",
        "        sets_of_indices = [set(inverted_index[term]) for term in search_terms if term in inverted_index]\n",
        "        if not sets_of_indices:\n",
        "            return pd.DataFrame()  # Return an empty DataFrame if no indices found\n",
        "\n",
        "        # Intersect all sets of indices to find common ones\n",
        "        valid_indices = set.intersection(*sets_of_indices)\n",
        "\n",
        "        # Convert the set of valid indices to a list\n",
        "        valid_indices_list = list(valid_indices)\n",
        "\n",
        "        # Select and return the relevant recipes using the valid indices\n",
        "        return recipe_data.loc[valid_indices_list, ['title', 'directions']]\n",
        "    except KeyError as e:\n",
        "        # Handle the case where one or more search terms are not in the matrix\n",
        "        print(f\"Warning: {str(e).strip('[]')} not found in recipe terms.\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if any term is not found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search Results:\n",
            "                                title  \\\n",
            "38918          Golden Harvest Muffins   \n",
            "13836        Country Fair Corn Sticks   \n",
            "31762               Cinnamon Nut Cake   \n",
            "43028        Baked Apple French Toast   \n",
            "13333        Deep Dark Chocolate Cake   \n",
            "...                               ...   \n",
            "30683              Fudge Pudding Cake   \n",
            "10204                 Company Muffins   \n",
            "4582                   Cornbread Cake   \n",
            "10732                   Pumpkin Bread   \n",
            "26093  Portzeln (New Year'S Fritters)   \n",
            "\n",
            "                                              directions  \n",
            "38918  heat oven quart bowl combine flour sugar bakin...  \n",
            "13836  preheat oven grease well corn stick mold place...  \n",
            "31762  preheat oven mix cake mix oil egg sugar sour c...  \n",
            "43028  preheat oven spray x inch baking pan dish vege...  \n",
            "13333  heat oven grease flour inch baking pan large b...  \n",
            "...                                                  ...  \n",
            "30683  large mixing bowl stir together flour cup suga...  \n",
            "10204  preheat oven combine flour oat bran baking sod...  \n",
            "4582   grease x inch pan mix together bisquick cornme...  \n",
            "10732  large bowl combine pancake mix sugar cinnamon ...  \n",
            "26093  sprinkle yeast water stir let stand bubbly lar...  \n",
            "\n",
            "[87 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "inverted_index = create_inverted_index_map_reduce(recipes)\n",
        "search_results = inverted_index_search(recipes, inverted_index, search_terms)\n",
        "print(\"Search Results:\")\n",
        "print(search_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inverted Index using Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART A:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating the tree classes from scratch:\n",
        "#The node class initializes a tree node contatining a word and its respective list of document ids\n",
        "class TreeNode:\n",
        "    def __init__(self, word, document_id):\n",
        "        self.word = word\n",
        "        self.document_ids = set([document_id])\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "\n",
        "#This class implements the actual binary tree BST with each left node having the word that is smaller alphabetically\n",
        "class BinaryTree:\n",
        "    def __init__(self):\n",
        "        self.root = None\n",
        "\n",
        "    def insert(self, word, document_id):\n",
        "        if not self.root: #If the root is empty\n",
        "            self.root = TreeNode(word, document_id) #Initialize the root with the node containing the word\n",
        "        else:\n",
        "            self._insert_rec(self.root, word, document_id)\n",
        "\n",
        "    #Locates and puts the given word and its respective document id in its correct position or append the existing one with the id\n",
        "    def _insert_rec(self, node, word, document_id):\n",
        "        if word == node.word:   #If current node contains the given word\n",
        "            node.document_ids.add(document_id)\n",
        "        elif word < node.word:  #If word is alphabetically less than the word at current node -> go left\n",
        "            if node.left is None:   #If nothing is on the left -> here it is\n",
        "                node.left = TreeNode(word, document_id)\n",
        "            else:   #If not, do this thing all over again recursively on the left node\n",
        "                self._insert_rec(node.left, word, document_id)\n",
        "        else: #If word is alphabetically more than the word at current node -> go right\n",
        "            if node.right is None:  #If nothing is on the right -> here it is\n",
        "                node.right = TreeNode(word, document_id)\n",
        "            else:   #If not, do this thing all over again recursively on the right node\n",
        "                self._insert_rec(node.right, word, document_id)\n",
        "\n",
        "    #Returns document ids given a word or None otherwise\n",
        "    def search(self, word):\n",
        "        return self._search_recursively(self.root, word)\n",
        "\n",
        "    #Recursively searches for a word to return its document ids if exist, None if not\n",
        "    def _search_recursively(self, node, word):\n",
        "        if node is None:   #If current node is null\n",
        "            return None\n",
        "        elif word == node.word: #If current node contains our word\n",
        "            return node.document_ids\n",
        "        elif word < node.word:  #If word is alphabetically less than the word at current node ->\n",
        "            return self._search_recursively(node.left,  word)  #Pass left node to this function\n",
        "        else:\n",
        "            return self._search_recursively(node.right, word) #Pass right node to this function\n",
        "        \n",
        "\n",
        "\n",
        "#Method to read through the recipe and add its word to a single overall BTS as implemented with the classes above\n",
        "def build_tree_index(data):\n",
        "    tree = BinaryTree() #Initialize tree\n",
        "    for index, row in data.iterrows():\n",
        "        words = row[\"directions\"].split() #Make arrays of words for each direction entry\n",
        "        for word in words:\n",
        "            tree.insert(word, index)\n",
        "    return tree\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART B:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_tree_index(tree, query_words):\n",
        "    if not query_words:\n",
        "        return pd.DataFrame()  # If the query is empty, return empty df\n",
        "\n",
        "    # Get document ids for each word in the query\n",
        "    doc_sets = []\n",
        "    for word in query_words:\n",
        "        result = tree.search(word)  # Search the tree for the word and get the document id list back\n",
        "        if result:  # If the return is not None\n",
        "            doc_sets.append(result)\n",
        "    \n",
        "    # Intersect all document sets\n",
        "    if doc_sets:\n",
        "        valid_indices = set.intersection(*doc_sets)\n",
        "        # exchange the set of all valid indices to a list before using it for indexing to prevent error\n",
        "        valid_indices_list = list(valid_indices)\n",
        "        return recipes.loc[valid_indices_list, ['title', 'directions']]\n",
        "    else:\n",
        "        return pd.DataFrame()  #if no words are found, return empty df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART C:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                title  \\\n",
            "38918          Golden Harvest Muffins   \n",
            "13836        Country Fair Corn Sticks   \n",
            "31762               Cinnamon Nut Cake   \n",
            "43028        Baked Apple French Toast   \n",
            "13333        Deep Dark Chocolate Cake   \n",
            "...                               ...   \n",
            "30683              Fudge Pudding Cake   \n",
            "10204                 Company Muffins   \n",
            "4582                   Cornbread Cake   \n",
            "10732                   Pumpkin Bread   \n",
            "26093  Portzeln (New Year'S Fritters)   \n",
            "\n",
            "                                              directions  \n",
            "38918  heat oven quart bowl combine flour sugar bakin...  \n",
            "13836  preheat oven grease well corn stick mold place...  \n",
            "31762  preheat oven mix cake mix oil egg sugar sour c...  \n",
            "43028  preheat oven spray x inch baking pan dish vege...  \n",
            "13333  heat oven grease flour inch baking pan large b...  \n",
            "...                                                  ...  \n",
            "30683  large mixing bowl stir together flour cup suga...  \n",
            "10204  preheat oven combine flour oat bran baking sod...  \n",
            "4582   grease x inch pan mix together bisquick cornme...  \n",
            "10732  large bowl combine pancake mix sugar cinnamon ...  \n",
            "26093  sprinkle yeast water stir let stand bubbly lar...  \n",
            "\n",
            "[87 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "#This method uses inverted index using a tree to search the dataframe for terms given as a list\n",
        "def search_tree_and_print(data, search_terms):\n",
        "    tree_index = build_tree_index(data)\n",
        "    result = search_tree_index(tree_index, search_terms)\n",
        "    print (result)\n",
        "\n",
        "\n",
        "search_tree_and_print(recipes, search_terms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART D:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 0.002095937728881836 seconds ---\n",
            "--- 0.001809835433959961 seconds ---\n",
            "--- 0.0015528202056884766 seconds ---\n",
            "\n",
            "the average latency:  0.0018735726674397786\n"
          ]
        }
      ],
      "source": [
        "latency = []\n",
        "\n",
        "tree_index = build_tree_index(recipes)\n",
        "\n",
        "#Calculate the latency time 3 times and getting the averages: From Tutorial 1\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "search_tree_index(tree_index, search_terms)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "latency.append(time.time() - start_time)\n",
        "\n",
        "start_time = time.time()\n",
        "search_tree_index(tree_index, search_terms)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "latency.append(time.time() - start_time)\n",
        "\n",
        "start_time = time.time()\n",
        "search_tree_index(tree_index, search_terms)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "latency.append(time.time() - start_time)\n",
        "\n",
        "# average latency of the times\n",
        "latency = sum(latency)/len(latency)\n",
        "print(\"\\nthe average latency: \",latency)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Jaccard Similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART A:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                 title  \\\n",
            "5120               Fresh Apple Muffins   \n",
            "18944                German Apple Cake   \n",
            "35457  Parmesan Pork Chops With Apples   \n",
            "18304                 Fresh Apple Cake   \n",
            "38918           Golden Harvest Muffins   \n",
            "...                                ...   \n",
            "30711           Fresh Apple Pound Cake   \n",
            "48888                 Fresh Apple Cake   \n",
            "31993         Fresh Apple Or Pear Cake   \n",
            "11389                 Fresh Apple Cake   \n",
            "12030                 Marinated Shrimp   \n",
            "\n",
            "                                              directions  jaccard_similarity  \n",
            "5120   preheat oven large bowl mix together oil sugar...            0.066667  \n",
            "18944  sift together flour baking soda salt cinnamon ...            0.120000  \n",
            "35457  combine egg milk set aside combine flour parme...            0.069767  \n",
            "18304  combine oil sugar blend add egg one time add v...            0.107143  \n",
            "38918  heat oven quart bowl combine flour sugar bakin...            0.093750  \n",
            "...                                                  ...                 ...  \n",
            "30711  combine oil sugar egg beat medium speed minute...            0.111111  \n",
            "48888  combine sugar flour salt soda baking powder ci...            0.130435  \n",
            "31993  combine sugar oil vanilla egg lemon juice salt...            0.115385  \n",
            "11389  combine prepared apple sugar large mixing bowl...            0.103448  \n",
            "12030  combine shrimp carrot onion pepper set aside m...            0.093750  \n",
            "\n",
            "[62 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "#This method calculates the Jaccard similarity between two sets\n",
        "def jaccard_similarity(set1, set2):\n",
        "    #calculating the intersection and union part of the equation to calculate the formula\n",
        "    intersection_part = set1.intersection(set2)\n",
        "    union_part = set1.union(set2)\n",
        "    if not union_part:\n",
        "        return 0  # Avoid / by zero\n",
        "    similarity = len(intersection_part) / len(union_part)   #Jaccard formula\n",
        "    return similarity\n",
        "\n",
        "def jaccard_similarity_search(recipe_data, inverted_index, search_terms):\n",
        "    try:\n",
        "        sets_of_indices = [set(inverted_index[term]) for term in search_terms if term in inverted_index]\n",
        "        if not sets_of_indices:\n",
        "            return pd.DataFrame() \n",
        "        valid_indices = set.intersection(*sets_of_indices)\n",
        "        valid_indices_list = list(valid_indices)\n",
        "        result = recipe_data.loc[valid_indices_list, ['title', 'directions']]\n",
        "        result['jaccard_similarity'] = result['directions'].apply(lambda x: jaccard_similarity(set(x.split()), set(search_terms)))\n",
        "\n",
        "        return result\n",
        "    except KeyError as e:\n",
        "        print(f\"Warning: {str(e).strip('[]')} not found in recipe terms.\")\n",
        "        return pd.DataFrame() \n",
        "    \n",
        "\n",
        "search_terms = [\"apple\", \"oil\", \"combine\"]\n",
        "#Please refer to above cells for where inverted_index\n",
        "result = jaccard_similarity_search (recipes, inverted_index, search_terms)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART B:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 0.06626391410827637 seconds ---\n",
            "--- 0.0042951107025146484 seconds ---\n",
            "--- 0.003253936767578125 seconds ---\n",
            "\n",
            "the average latency:  0.024831612904866535\n"
          ]
        }
      ],
      "source": [
        "#Calculating and printing the service latency of the Jaccard Similarity search implementation\n",
        "def calc_latency(data, inverted_index, search_terms):\n",
        "    latency = []\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "    jaccard_similarity_search (recipes, inverted_index, search_terms)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    latency.append(time.time() - start_time)\n",
        "\n",
        "    start_time = time.time()\n",
        "    jaccard_similarity_search (recipes, inverted_index, search_terms)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    latency.append(time.time() - start_time)\n",
        "\n",
        "    start_time = time.time()\n",
        "    jaccard_similarity_search (recipes, inverted_index, search_terms)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    latency.append(time.time() - start_time)\n",
        "\n",
        "    # average latency of the times\n",
        "    latency = sum(latency)/len(latency)\n",
        "    print(\"\\nthe average latency: \",latency)\n",
        "\n",
        "calc_latency (recipes, inverted_index, search_terms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                          title  TFIDF_similarity\n",
            "27255                             Ice Cream Pie          1.085778\n",
            "19470                      Chocolate Milk Shake          1.069135\n",
            "19701  Easy Chocolate Cake(Needs No Frosting)            1.021685\n",
            "32066                       Fruit Cocktail Cake          1.018895\n",
            "12717                 Strawberry Ice Cream Cake          1.014370\n",
            "...                                         ...               ...\n",
            "19017                  Caramel Chews(Unbaked)            0.000000\n",
            "19018                  Slow-Cooked Pepper Steak          0.000000\n",
            "19020                               Party Salad          0.000000\n",
            "19021                Broccoli Cauliflower Salad          0.000000\n",
            "49999                       Granny Ebert'S Hash          0.000000\n",
            "\n",
            "[50000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "#For this part I am using the TfidfVectorizer library from SKlearn which I found by searching \"TFIDF Vectorizer\".git\n",
        "#I used the following link as reference:\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \n",
        "#Please find the imports in the Imports section of Q1\n",
        "\n",
        "#This method vectorizes a given df according to TFIDF\n",
        "def make_tdidfIndex(data):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(data[\"directions\"]) #Make up the matrix using the vectorizer\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "\n",
        "def tfidf_similarity_search(data, query):\n",
        "    vectorizer, tfidf_matrix = make_tdidfIndex(recipes)\n",
        "\n",
        "    # Make the query a TF-IDF vector of the given corpus\n",
        "    query_vector = vectorizer.transform([query])\n",
        "\n",
        "    # make the list and  dense array\n",
        "    query_arr = query_vector.toarray()\n",
        "    corpus_arr = tfidf_matrix.toarray()\n",
        "    \n",
        "    result = []\n",
        "    \n",
        "    for index, doc_vector in enumerate(corpus_arr):\n",
        "        # only add scores for terms that appear in the query\n",
        "        similarity = sum(doc_vector[query_arr[0] > 0])\n",
        "        result.append({\"title\": data.iloc[index][\"title\"], \"TFIDF_similarity\": similarity})\n",
        "\n",
        "    # turn the list of dictionariess into dfs\n",
        "    result_df = pd.DataFrame(result)\n",
        "\n",
        "    #sort results by TFIDF in decreasing order\n",
        "    result_df.sort_values(by=\"TFIDF_similarity\", ascending=False, inplace=True)\n",
        "    \n",
        "    return result_df\n",
        "\n",
        "\n",
        "\n",
        "query = \"chocolate ice cream cake\"\n",
        "results = tfidf_similarity_search(recipes, query)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART B:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 2.604679822921753 seconds ---\n",
            "--- 2.349626064300537 seconds ---\n",
            "--- 2.4054667949676514 seconds ---\n",
            "\n",
            "the average latency:  2.4536872704823813\n"
          ]
        }
      ],
      "source": [
        "#Calculating and printing the service latency of the TF-IDF similarity search implementation\n",
        "def calc_latency_tfidf (data, query):\n",
        "    latency = []\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "    tfidf_similarity_search (data, query)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    latency.append(time.time() - start_time)\n",
        "\n",
        "    start_time = time.time()\n",
        "    tfidf_similarity_search (data, query)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    latency.append(time.time() - start_time)\n",
        "\n",
        "    start_time = time.time()\n",
        "    tfidf_similarity_search (data, query)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    latency.append(time.time() - start_time)\n",
        "\n",
        "    # average latency of the times\n",
        "    latency = sum(latency)/len(latency)\n",
        "    print(\"\\nthe average latency: \",latency)\n",
        "\n",
        "calc_latency_tfidf (recipes, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Search Index using Whoosh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART A:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "schema = Schema(\n",
        "    title=TEXT(stored=True),  # Store the title to display in search results, and make it searchable\n",
        "    directions=TEXT(analyzer=StemmingAnalyzer(), stored=True)  # Store and index the directions for searching\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART B:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "#FROM TUTORIAL 1:\n",
        "# Making sure that the index directory exists - creating it if not\n",
        "if not os.path.exists(\"indexdir\"):\n",
        "    os.mkdir(\"indexdir\")\n",
        "\n",
        "# Create index schema and an index using the schema\n",
        "index = create_in(\"indexdir\", schema)\n",
        "\n",
        "def write_to_index(data):\n",
        "    #Create a writer object\n",
        "    writer = index.writer()\n",
        "    # write the data to the index\n",
        "    for i, row in recipes.iterrows():\n",
        "        writer.add_document(title=row[\"title\"], directions=row[\"directions\"])\n",
        "    # Commit changes to the writer\n",
        "    writer.commit()\n",
        "\n",
        "# call the function to write to the index\n",
        "write_to_index(recipes[:10000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART C:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: \n",
            "\n",
            "chocolate ice cream cake\n",
            "\n",
            "Top 5 Results:\n",
            "Title: Triple Mint Ice-Cream Angel Dessert\n",
            "Directions: split cake horizontally four equal layer place bottom cake layer serving plate spread top layer half chocolate mint ice cream within inch edge top second cake layer cover freeze minute firm spread second cake layer pink peppermint ice cream add third cake layer cover freeze minute firm spread third cake layer remaining chocolate mint ice cream top remaining cake layer freeze firm\n",
            "Score: 68.36586268081896\n",
            "Search time: 0.047796010971069336\n",
            "\n",
            "\n",
            "Title: Ice Cream Cake\n",
            "Directions: layer crushed oreo bottom oblong dish pan lace chocolate syrup add layer chocolate ice cream sprinkle crushed oreo ice cream layer add layer vanilla ice cream cover cool whip decorate sprinkle syrup oreo\n",
            "Score: 44.836604949704785\n",
            "Search time: 0.047796010971069336\n",
            "\n",
            "\n",
            "Title: Chocolate Devil'S Food Cake\n",
            "Directions: cream crisco sugar add egg sift add flour spice together salt dissolve chocolate hot coffee soda buttermilk stir foam add ingredient vanilla cake coloring bake minute ice favorite icing chocolate cream cheese minute icing\n",
            "Score: 39.4409706057793\n",
            "Search time: 0.047796010971069336\n",
            "\n",
            "\n",
            "Title: Special Chocolate Cake\n",
            "Directions: mix german chocolate cake mix directed box bake inch x inch pan cool completely punch hole top cake pour condensed milk cake pour cream coconut cake ice small cool whip crush butterfingers sprinkle top refrigerate\n",
            "Score: 34.7673857357014\n",
            "Search time: 0.047796010971069336\n",
            "\n",
            "\n",
            "Title: Grandma'S Sour Cream Chocolate Cake\n",
            "Directions: preheat oven grease flour inch cake pan cream butter margarine shortening brown sugar vanilla add egg cream light fluffy blend melted chocolate sour cream beat minute medium speed mix well stroke hand blend sifted dry ingredient water beat minute stroke hand bake minute cool pan minute ice creamy vanilla frosting\n",
            "Score: 33.328383491842324\n",
            "Search time: 0.047796010971069336\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#For this part I used Whoosh's built in Scoring class, please find it imported in the Imports section of Q1 with all\n",
        "#other imports. I found information on it by searching \"Whoosh score tf idf\" in Google and found information on it\n",
        "#from https://whoosh.readthedocs.io/en/latest/api/scoring.html \n",
        "#Also I found https://whoosh.readthedocs.io/en/latest/parsing.html and MultifieldParser on Whoosh's website. It is imported above as well\n",
        "\n",
        "#Searches and indexes and calculates the 5 most relavant docs to query based on TF IDF scoring. If print is 1, it prints them, if 0, it only returns\n",
        "#the search time\n",
        "def search_index(query, to_print):\n",
        "    # Open the existing index directory\n",
        "    index = open_dir(\"indexdir\")\n",
        "\n",
        "    # create a query parser for the content field in the index schema\n",
        "    query_parser = MultifieldParser([\"title\", \"directions\"], schema=index.schema)\n",
        "\n",
        "    # Parse the query\n",
        "    query = query_parser.parse(query)\n",
        "\n",
        "    #define our TF_IDF searcher from whoosh\n",
        "    searcher = index.searcher(weighting=scoring.TF_IDF())\n",
        "\n",
        "    #THIS IS FOR PART E: calculating time for searching and indexing\n",
        "    start_time = time.time()\n",
        "\n",
        "    # search the index\n",
        "    results = searcher.search(query, limit = 5)\n",
        "\n",
        "    search_time = time.time() - start_time\n",
        "\n",
        "    \n",
        "\n",
        "    if (to_print != 0):\n",
        "        #Printing the results\n",
        "        for result in results:\n",
        "            print(\"Title:\", result[\"title\"])\n",
        "            print(\"Directions:\", result[\"directions\"])\n",
        "            print(\"Score:\", result.score)\n",
        "            print(\"Search time:\", search_time)\n",
        "            print(\"\\n\")\n",
        "\n",
        "    \n",
        "    else:   \n",
        "        return search_time\n",
        "\n",
        "# Example usage of the search function\n",
        "print(\"Query: \\n\")\n",
        "print(query)\n",
        "print (\"\\nTop 5 Results:\")\n",
        "search_index(query, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART C Justification for using TF-IDF: TF-IDF is ideal for finding the score of the importance of a query in a document as it both has the term frequency part which assesses the number of occurences of the terms within the document and also it has the Inverse document frequency which assesses the informativeness of the term within the document which in a sense normalizes the score given to the document based on the query. This makes it ideal as it does not allow certain not-so-informative terms that may occur a lot dominate the scoring as it normalizes their importance based on the IDF, while also taking the number of occurrences into account."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART D: ANSWERED IN PART C (See above). I just do a few queries for this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: \n",
            "\n",
            "sugar oil bowl milk\n",
            "\n",
            "Top 5 Results:\n",
            "Title: Cinnamon Rolls\n",
            "Directions: scald milk combine sugar oil salt bowl add milk stir dissolve cool lukewarm soften yeast water add milk mixture beat egg add milk mix add flour mix soft dough knead minute put greased bowl rise warm place hour roll oblong inch thick brush tablespoon sugar teaspoon cinnamon roll cut inch slice lay flat greased round pan rise hour bake minute\n",
            "Score: 24.945647150817244\n",
            "Search time: 0.0771479606628418\n",
            "\n",
            "\n",
            "Title: Doughnuts\n",
            "Directions: soften yeast water add teaspoon cup sugar set aside add fat shortening rest sugar salt hot milk stir sugar dissolved cool add egg stir softened yeast stir flour liquid ingredient well mixed turn dough onto lightly floured board knead quickly smooth elastic form smooth ball place greased bowl turn grease surface cover let rise warm place double bulk hour turn onto board knead well roll cut doughnut cutter place waxed paper let rise double bulk cook cooking oil brown still hot roll powdered sugar icing made mixing box powdered sugar enough milk make thin enough dip doughnut\n",
            "Score: 23.07137468078362\n",
            "Search time: 0.0771479606628418\n",
            "\n",
            "\n",
            "Title: No Fail Yeast Rolls\n",
            "Directions: stir yeast salt sugar flour heat milk hot touch add milk oil dry ingredient beating well blended minute spoon add much flour needed turn onto floured surface knead smooth place greased oiled bowl cover towel let rise double punch turn onto floured surface roll cut biscuit cutter put oiled pan let rise double cook brown\n",
            "Score: 20.80852902475187\n",
            "Search time: 0.0771479606628418\n",
            "\n",
            "\n",
            "Title: Pumpkin-Pecan Bread (Low-Fat)\n",
            "Directions: mix flour baking powder baking soda salt sugar cinnamon nutmeg large bowl make well center combine pumpkin milk oil egg substitute pecan medium bowl mix well pour well flour mixture stir moistened spoon x inch loaf pan sprayed nonstick oil bake minute cool remove pan sprinkle confectioner sugar desired serve cream cheese\n",
            "Score: 19.759977950009326\n",
            "Search time: 0.0771479606628418\n",
            "\n",
            "\n",
            "Title: Portzeln (New Year'S Fritters)\n",
            "Directions: sprinkle yeast water stir let stand bubbly large bowl beat egg frothy add sugar continue beating thick lemon colored add cooled milk cream yeast mixture gradually add flour beating well addition stir raisin cover bowl let rise stir dough large spoon let rise drop batter tablespoonful hot oil fry lightly browned turn brown side often turn hence portzeln tumbler roll sugar serve sugar side\n",
            "Score: 18.65854827414861\n",
            "Search time: 0.0771479606628418\n",
            "\n",
            "\n",
            "Query: \n",
            "\n",
            "cheese burger\n",
            "\n",
            "Top 5 Results:\n",
            "Title: Spinach Casserole(Your Kids Will Never Know!)  \n",
            "Directions: thaw spinach brown burger onion remove excess grease bowl combine soup sour cream oregano garlic salt rice add spinach burger put x inch pan bake minute top mozzarella cheese sprinkle parmesan cheese bake another minute cheese begin brown\n",
            "Score: 24.888737892339307\n",
            "Search time: 0.005791902542114258\n",
            "\n",
            "\n",
            "Title: Redi-Cheese Burger\n",
            "Directions: mix ingredient well make burger bake oven fry serve roll alfalfa sprout slice red onion\n",
            "Score: 21.014831709553334\n",
            "Search time: 0.005791902542114258\n",
            "\n",
            "\n",
            "Title: El Paso Burgers\n",
            "Directions: place meat large bowl add onion green pepper taco mix combine thoroughly shape burger place onto well greased oven broiling pan spread salsa top bake done place slice cheese burger return oven long enough melt serve bun\n",
            "Score: 19.566222570011515\n",
            "Search time: 0.005791902542114258\n",
            "\n",
            "\n",
            "Title: Low Calorie Pizza\n",
            "Directions: brown burger drain grease mix tomato paste italian seasoning onion flake garlic salt burger spread pita bread sprinkle cheese top\n",
            "Score: 19.566222570011515\n",
            "Search time: 0.005791902542114258\n",
            "\n",
            "\n",
            "Title: Noodles Mexicana\n",
            "Directions: large skillet brown burger onion pepper garlic oil stir salt corn olive tomato sauce cheese paprika cook noodle directed drain combine noodle burger mixture pour quart casserole cover bake minute\n",
            "Score: 19.566222570011515\n",
            "Search time: 0.005791902542114258\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query2 = \"sugar oil bowl milk\"\n",
        "print(\"Query: \\n\")\n",
        "print(query2)\n",
        "print (\"\\nTop 5 Results:\")\n",
        "search_index(query2, 1)\n",
        "\n",
        "query3 = \"cheese burger\"\n",
        "print(\"Query: \\n\")\n",
        "print(query3)\n",
        "print (\"\\nTop 5 Results:\")\n",
        "search_index(query3, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART E: Implemented part E.a in PART C's function. Averaging:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.07534978389739991\n"
          ]
        }
      ],
      "source": [
        "#Taking average of 10 runs\n",
        "total_time = 0\n",
        "query4 = \"sugar oil bowl milk\"\n",
        "for i in range(10):\n",
        "    total_time = total_time + search_index(query4, 0)\n",
        "\n",
        "print(total_time/10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Performance Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PART A: The average service latency for the Inverted index using trees was 0.0018735726674397786, Jaccard Similarity was 0.024831612904866535, TF-IDF without using cosine similarity was 2.4536872704823813, and the average service latency of Whoosh using TF-IDF was 0.07534978389739991. This makes Inverted Index using trees the fastest by far followed by Jaccard Similarity and Whoosh. All methods were given the same dataframe and the result was calculated as an average of multiple runs. This is a fairly reliable method as it is the same method of testing averaged over multiple time frames. I believe the reason behind the inverted index being faster for the purpose of this dataframe is that after creating a tree it is very efficient and to traverse the tree to find the specific word. But it does need an initial setting up of the tree which could be time consuming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most of the search methods returned the exact same thing, some of them just returned a limited amount of result which could be more helpful in certain situations like in implementing search engines but generally the detected results were similar. TF-IDF methods returned better results as they take both the relevance and importance of each word into account."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
